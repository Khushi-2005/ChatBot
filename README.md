ğŸ“„ RAG PDF Chatbot
This project is a Retrieval-Augmented Generation (RAG) based chatbot that allows you to chat with your PDFs. It uses HuggingFace sentence-transformer embeddings and ChromaDB for semantic search, combined with a local LLM for generating context-aware answers.
ğŸš€ Features
ğŸ” Upload & Ingest PDFs â€“ Split documents into chunks and store them in ChromaDB.
ğŸ¤– Local LLM Response â€“ Uses a local language model for answering queries (no OpenAI API required).
ğŸ§  Conversational Memory â€“ Maintains chat history per session with a session ID.
ğŸ“‘ Citations â€“ Provides clickable links to the PDF page numbers where the answer was generated from.
âš¡ Fast & Private â€“ Runs entirely on your local machine.
ğŸ› ï¸ Tech Stack
Frontend: Streamlit
Vector Database: ChromaDB
Embeddings: HuggingFace (all-MiniLM-L6-v2)
LLM: Local model (can be replaced with GPT4All, LLaMA, or other HuggingFace models)
Environment Management: Python + dotenv
ğŸ“‚ Project Structure
â”œâ”€â”€ app.py          # Streamlit app (frontend)  
â”œâ”€â”€ backend.py      # Query handling, memory, and LLM pipeline  
â”œâ”€â”€ ingest.py       # PDF ingestion and vectorstore creation  
â”œâ”€â”€ utils.py        # Helpers & environment setup  
â”œâ”€â”€ vectorstore/    # ChromaDB persistent storage  
â”œâ”€â”€ requirements.txt
âš¡ How It Works
Run ingest.py to load your PDF and create a vectorstore.
Start the chatbot with:
streamlit run app.py
Ask questions about your PDF and get context-aware answers with citation
## ğŸ“¸ Screenshots

### Chat Interface
![Chat Interface](images/chatbot_ui.png)

### PDF Upload
![PDF Upload](images/Chatbot.png)



